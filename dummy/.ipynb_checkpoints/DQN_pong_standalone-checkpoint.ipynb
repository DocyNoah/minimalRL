{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17  \n",
    "- https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af  \n",
    "- https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c\n",
    "- https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "\n",
    "def seed_everything(env, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        \n",
    "        self.out = 0\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = self.fc(x)\n",
    "        return self.out\n",
    "    \n",
    "    def train(self):\n",
    "        x = torch.rand(4)\n",
    "        x = torch.tensor(x, requires_grad=True)\n",
    "                \n",
    "        loss = self.out\n",
    "        loss /= 10\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_test(env, device=torch.device(\"cpu\")):\n",
    "    print(device)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    model = TestModel()\n",
    "\n",
    "    print(\"random.random() :\", random.random())\n",
    "    print(\"np.random.rand() :\", np.random.rand())\n",
    "    print(\"torch.rand(1) :\", torch.rand(1))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for i in range(100):\n",
    "        x = torch.rand(4)\n",
    "        x = torch.tensor(x, requires_grad=True)\n",
    "\n",
    "        model(x)\n",
    "        loss = model.train()\n",
    "\n",
    "    print(\"model(x) :\", model(x))\n",
    "    print(\"loss :\", loss)\n",
    "\n",
    "\n",
    "    print(np.random.choice(100, 7, replace=False))\n",
    "    \n",
    "    env.reset()\n",
    "    for _ in range(10):\n",
    "        env.reset()\n",
    "        print(env.action_space.sample(), end=\" \")\n",
    "        \n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed_everything(env)\n",
    "\n",
    "device = device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed_test(env, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed_everything(env)\n",
    "\n",
    "device = device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed_test(env, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "## OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "bd0f45bc-3137-4a02-ef13-e71054a16753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QDaXip14JBv",
    "outputId": "cf5337c3-c39d-4774-eb88-f9b094c2e8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uzLQLz04z2i",
    "outputId": "2edf4a8b-3cad-4f76-befa-284baee687c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjUM99rEKFNt",
    "outputId": "d2ddfec6-12b1-45a4-b599-b696672e080c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 05 03:57:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.81       Driver Version: 456.81       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX          WDDM  | 00000000:02:00.0  On |                  N/A |\n",
      "|  0%   45C    P0   149W / 280W |   3142MiB / 24576MiB |     64%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX          WDDM  | 00000000:21:00.0  On |                  N/A |\n",
      "|  0%   41C    P2    66W / 280W |   1868MiB / 24576MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1796    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2668    C+G   ...902.55\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      3048    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      4984    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      5312    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      6720    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7624    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      7732    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8844    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10312    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10968    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10984    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     12784    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     12792    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     13416    C+G   ...xLauncher\\MaxLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     13688    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     13904    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14592    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16296    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     16840    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A     22224    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     22556    C+G   ...nvs\\rlenv_cuda\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     23336    C+G   ...2\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    1   N/A  N/A      1796    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A      3048    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     12784    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     12792    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     13416    C+G   ...xLauncher\\MaxLauncher.exe    N/A      |\n",
      "|    1   N/A  N/A     22556      C   ...nvs\\rlenv_cuda\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taYi5LZnIOqz",
    "outputId": "e59ca720-e4ca-4fdc-a5b7-795f4359de43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "# MEAN_REWARD_BOUND = 19.0           \n",
    "MEAN_REWARD_BOUND = 15.0\n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        done_reward = None\n",
    "        \n",
    "#         rr = np.random.random()\n",
    "#         print(\"np.random.random() :\", rr)\n",
    "        if np.random.random() < epsilon:\n",
    "#         if rr < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "#             print(\"state_v.dtype : {}\".format(state_v.dtype))\n",
    "            q_vals_v = net(state_v)\n",
    "#             print(\"q_vals_v.type : {}\".format(q_vals_v.type))\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "#             print(\"act_v.dtype : {}\".format(act_v.dtype))\n",
    "            action = int(act_v.item())\n",
    "#             print(\"type(action) : {}\".format(type(action)))\n",
    "\n",
    "#         print(\"action :\", action)\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "#         print(new_state.shape)\n",
    "#         print(new_state[3, 30:70:5, 8])\n",
    "        \n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipurwYpa6iKn",
    "outputId": "51833adf-bf0a-440b-bb0b-9f12b587f412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2021-08-05 03:57:57.444523\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEoc2PWmM2mu",
    "outputId": "a6e88222-0925-41c5-c20f-2daa316d386e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n",
      "Frame    822, Episode    1, Mean Reward -21.000, Loss 0.000, Epsilon 98.77%, Elapsed Time 02s\n",
      "Best mean reward updated -21.000\n",
      "Frame   1752, Episode    2, Mean Reward -20.500, Loss 0.000, Epsilon 97.41%, Elapsed Time 00s\n",
      "Best mean reward updated -20.500\n",
      "Frame   2574, Episode    3, Mean Reward -20.667, Loss 0.000, Epsilon 96.21%, Elapsed Time 00s\n",
      "Frame   3512, Episode    4, Mean Reward -20.500, Loss 0.000, Epsilon 94.87%, Elapsed Time 00s\n",
      "Frame   4302, Episode    5, Mean Reward -20.600, Loss 0.000, Epsilon 93.75%, Elapsed Time 00s\n",
      "Frame   5064, Episode    6, Mean Reward -20.667, Loss 0.000, Epsilon 92.69%, Elapsed Time 00s\n",
      "Frame   5961, Episode    7, Mean Reward -20.571, Loss 0.000, Epsilon 91.45%, Elapsed Time 00s\n",
      "Frame   6845, Episode    8, Mean Reward -20.625, Loss 0.000, Epsilon 90.24%, Elapsed Time 00s\n",
      "Frame   7727, Episode    9, Mean Reward -20.667, Loss 0.000, Epsilon 89.06%, Elapsed Time 00s\n",
      "Frame   8489, Episode   10, Mean Reward -20.700, Loss 0.000, Epsilon 88.04%, Elapsed Time 00s\n",
      "Frame   9311, Episode   11, Mean Reward -20.727, Loss 0.000, Epsilon 86.96%, Elapsed Time 00s\n",
      "Frame  10073, Episode   12, Mean Reward -20.750, Loss 0.031, Epsilon 85.98%, Elapsed Time 01s\n",
      "Frame  10835, Episode   13, Mean Reward -20.769, Loss 0.030, Epsilon 85.00%, Elapsed Time 11s\n",
      "Frame  11957, Episode   14, Mean Reward -20.643, Loss 0.030, Epsilon 83.58%, Elapsed Time 17s\n",
      "Frame  12844, Episode   15, Mean Reward -20.600, Loss 0.031, Epsilon 82.48%, Elapsed Time 13s\n",
      "Frame  13694, Episode   16, Mean Reward -20.625, Loss 0.030, Epsilon 81.43%, Elapsed Time 12s\n",
      "Frame  14822, Episode   17, Mean Reward -20.529, Loss 0.031, Epsilon 80.06%, Elapsed Time 17s\n",
      "Frame  15584, Episode   18, Mean Reward -20.556, Loss 0.001, Epsilon 79.16%, Elapsed Time 11s\n",
      "Frame  16498, Episode   19, Mean Reward -20.579, Loss 0.056, Epsilon 78.08%, Elapsed Time 13s\n",
      "Frame  17288, Episode   20, Mean Reward -20.600, Loss 0.023, Epsilon 77.16%, Elapsed Time 11s\n",
      "Frame  18248, Episode   21, Mean Reward -20.571, Loss 0.000, Epsilon 76.05%, Elapsed Time 14s\n",
      "Frame  19216, Episode   22, Mean Reward -20.591, Loss 0.008, Epsilon 74.96%, Elapsed Time 14s\n",
      "Frame  20038, Episode   23, Mean Reward -20.609, Loss 0.012, Epsilon 74.04%, Elapsed Time 12s\n",
      "Frame  21012, Episode   24, Mean Reward -20.542, Loss 0.001, Epsilon 72.97%, Elapsed Time 14s\n",
      "Frame  21993, Episode   25, Mean Reward -20.520, Loss 0.020, Epsilon 71.90%, Elapsed Time 14s\n",
      "Frame  23050, Episode   26, Mean Reward -20.462, Loss 0.038, Epsilon 70.77%, Elapsed Time 16s\n",
      "Best mean reward updated -20.462\n",
      "Frame  23952, Episode   27, Mean Reward -20.444, Loss 0.003, Epsilon 69.82%, Elapsed Time 13s\n",
      "Best mean reward updated -20.444\n",
      "Frame  25002, Episode   28, Mean Reward -20.429, Loss 0.008, Epsilon 68.73%, Elapsed Time 15s\n",
      "Best mean reward updated -20.429\n",
      "Frame  25959, Episode   29, Mean Reward -20.414, Loss 0.005, Epsilon 67.75%, Elapsed Time 14s\n",
      "Best mean reward updated -20.414\n",
      "Frame  26827, Episode   30, Mean Reward -20.400, Loss 0.001, Epsilon 66.87%, Elapsed Time 13s\n",
      "Best mean reward updated -20.400\n",
      "Frame  27667, Episode   31, Mean Reward -20.387, Loss 0.002, Epsilon 66.03%, Elapsed Time 12s\n",
      "Best mean reward updated -20.387\n",
      "Frame  28485, Episode   32, Mean Reward -20.406, Loss 0.001, Epsilon 65.23%, Elapsed Time 12s\n",
      "Frame  29294, Episode   33, Mean Reward -20.424, Loss 0.003, Epsilon 64.44%, Elapsed Time 11s\n",
      "Frame  30146, Episode   34, Mean Reward -20.441, Loss 0.004, Epsilon 63.62%, Elapsed Time 12s\n",
      "Frame  30936, Episode   35, Mean Reward -20.457, Loss 0.001, Epsilon 62.87%, Elapsed Time 11s\n",
      "Frame  31910, Episode   36, Mean Reward -20.444, Loss 0.003, Epsilon 61.96%, Elapsed Time 14s\n",
      "Frame  32822, Episode   37, Mean Reward -20.459, Loss 0.003, Epsilon 61.12%, Elapsed Time 13s\n",
      "Frame  33614, Episode   38, Mean Reward -20.474, Loss 0.003, Epsilon 60.40%, Elapsed Time 12s\n",
      "Frame  34634, Episode   39, Mean Reward -20.487, Loss 0.003, Epsilon 59.48%, Elapsed Time 15s\n",
      "Frame  35565, Episode   40, Mean Reward -20.500, Loss 0.001, Epsilon 58.66%, Elapsed Time 14s\n",
      "Frame  36487, Episode   41, Mean Reward -20.512, Loss 0.018, Epsilon 57.85%, Elapsed Time 13s\n",
      "Frame  37375, Episode   42, Mean Reward -20.500, Loss 0.018, Epsilon 57.08%, Elapsed Time 13s\n",
      "Frame  38368, Episode   43, Mean Reward -20.488, Loss 0.001, Epsilon 56.24%, Elapsed Time 15s\n",
      "Frame  39306, Episode   44, Mean Reward -20.500, Loss 0.004, Epsilon 55.46%, Elapsed Time 14s\n",
      "Frame  40458, Episode   45, Mean Reward -20.467, Loss 0.005, Epsilon 54.51%, Elapsed Time 17s\n",
      "Frame  41444, Episode   46, Mean Reward -20.478, Loss 0.013, Epsilon 53.70%, Elapsed Time 14s\n",
      "Frame  42485, Episode   47, Mean Reward -20.489, Loss 0.003, Epsilon 52.87%, Elapsed Time 15s\n",
      "Frame  43443, Episode   48, Mean Reward -20.500, Loss 0.005, Epsilon 52.12%, Elapsed Time 14s\n",
      "Frame  44732, Episode   49, Mean Reward -20.469, Loss 0.006, Epsilon 51.12%, Elapsed Time 19s\n",
      "Frame  45742, Episode   50, Mean Reward -20.460, Loss 0.003, Epsilon 50.35%, Elapsed Time 15s\n",
      "Frame  47250, Episode   51, Mean Reward -20.392, Loss 0.003, Epsilon 49.23%, Elapsed Time 23s\n",
      "Frame  48404, Episode   52, Mean Reward -20.385, Loss 0.003, Epsilon 48.38%, Elapsed Time 17s\n",
      "Best mean reward updated -20.385\n",
      "Frame  49464, Episode   53, Mean Reward -20.396, Loss 0.010, Epsilon 47.62%, Elapsed Time 17s\n",
      "Frame  50646, Episode   54, Mean Reward -20.389, Loss 0.004, Epsilon 46.78%, Elapsed Time 19s\n",
      "Frame  51487, Episode   55, Mean Reward -20.400, Loss 0.005, Epsilon 46.19%, Elapsed Time 13s\n",
      "Frame  52837, Episode   56, Mean Reward -20.339, Loss 0.012, Epsilon 45.27%, Elapsed Time 21s\n",
      "Best mean reward updated -20.339\n",
      "Frame  53886, Episode   57, Mean Reward -20.351, Loss 0.003, Epsilon 44.56%, Elapsed Time 16s\n",
      "Frame  55019, Episode   58, Mean Reward -20.345, Loss 0.017, Epsilon 43.81%, Elapsed Time 18s\n",
      "Frame  56284, Episode   59, Mean Reward -20.339, Loss 0.005, Epsilon 42.99%, Elapsed Time 21s\n",
      "Best mean reward updated -20.339\n",
      "Frame  57545, Episode   60, Mean Reward -20.333, Loss 0.006, Epsilon 42.18%, Elapsed Time 20s\n",
      "Best mean reward updated -20.333\n",
      "Frame  58623, Episode   61, Mean Reward -20.295, Loss 0.002, Epsilon 41.51%, Elapsed Time 17s\n",
      "Best mean reward updated -20.295\n",
      "Frame  59810, Episode   62, Mean Reward -20.274, Loss 0.004, Epsilon 40.77%, Elapsed Time 19s\n",
      "Best mean reward updated -20.274\n",
      "Frame  60819, Episode   63, Mean Reward -20.270, Loss 0.004, Epsilon 40.16%, Elapsed Time 16s\n",
      "Best mean reward updated -20.270\n",
      "Frame  61896, Episode   64, Mean Reward -20.281, Loss 0.011, Epsilon 39.52%, Elapsed Time 17s\n",
      "Frame  63051, Episode   65, Mean Reward -20.246, Loss 0.009, Epsilon 38.84%, Elapsed Time 18s\n",
      "Best mean reward updated -20.246\n",
      "Frame  64315, Episode   66, Mean Reward -20.258, Loss 0.004, Epsilon 38.11%, Elapsed Time 19s\n",
      "Frame  65592, Episode   67, Mean Reward -20.254, Loss 0.006, Epsilon 37.39%, Elapsed Time 19s\n",
      "Frame  66800, Episode   68, Mean Reward -20.265, Loss 0.003, Epsilon 36.71%, Elapsed Time 19s\n",
      "Frame  68047, Episode   69, Mean Reward -20.261, Loss 0.005, Epsilon 36.03%, Elapsed Time 20s\n",
      "Frame  69163, Episode   70, Mean Reward -20.257, Loss 0.005, Epsilon 35.44%, Elapsed Time 17s\n",
      "Frame  70451, Episode   71, Mean Reward -20.225, Loss 0.006, Epsilon 34.76%, Elapsed Time 19s\n",
      "Best mean reward updated -20.225\n",
      "Frame  71777, Episode   72, Mean Reward -20.222, Loss 0.004, Epsilon 34.07%, Elapsed Time 20s\n",
      "Best mean reward updated -20.222\n",
      "Frame  73244, Episode   73, Mean Reward -20.205, Loss 0.002, Epsilon 33.33%, Elapsed Time 22s\n",
      "Best mean reward updated -20.205\n",
      "Frame  74659, Episode   74, Mean Reward -20.162, Loss 0.006, Epsilon 32.63%, Elapsed Time 22s\n",
      "Best mean reward updated -20.162\n",
      "Frame  76346, Episode   75, Mean Reward -20.160, Loss 0.005, Epsilon 31.82%, Elapsed Time 26s\n",
      "Best mean reward updated -20.160\n",
      "Frame  78410, Episode   76, Mean Reward -20.118, Loss 0.003, Epsilon 30.85%, Elapsed Time 33s\n",
      "Best mean reward updated -20.118\n",
      "Frame  79902, Episode   77, Mean Reward -20.104, Loss 0.003, Epsilon 30.16%, Elapsed Time 23s\n",
      "Best mean reward updated -20.104\n",
      "Frame  81200, Episode   78, Mean Reward -20.103, Loss 0.013, Epsilon 29.58%, Elapsed Time 21s\n",
      "Best mean reward updated -20.103\n",
      "Frame  83100, Episode   79, Mean Reward -20.038, Loss 0.007, Epsilon 28.75%, Elapsed Time 31s\n",
      "Best mean reward updated -20.038\n",
      "Frame  84514, Episode   80, Mean Reward -20.025, Loss 0.004, Epsilon 28.15%, Elapsed Time 23s\n",
      "Best mean reward updated -20.025\n",
      "Frame  86056, Episode   81, Mean Reward -20.012, Loss 0.008, Epsilon 27.50%, Elapsed Time 24s\n",
      "Best mean reward updated -20.012\n",
      "Frame  87578, Episode   82, Mean Reward -20.012, Loss 0.004, Epsilon 26.88%, Elapsed Time 24s\n",
      "Best mean reward updated -20.012\n",
      "Frame  89383, Episode   83, Mean Reward -20.000, Loss 0.002, Epsilon 26.16%, Elapsed Time 30s\n",
      "Best mean reward updated -20.000\n",
      "Frame  90839, Episode   84, Mean Reward -19.976, Loss 0.014, Epsilon 25.60%, Elapsed Time 23s\n",
      "Best mean reward updated -19.976\n",
      "Frame  92435, Episode   85, Mean Reward -19.929, Loss 0.004, Epsilon 24.99%, Elapsed Time 26s\n",
      "Best mean reward updated -19.929\n",
      "Frame  94149, Episode   86, Mean Reward -19.895, Loss 0.003, Epsilon 24.36%, Elapsed Time 27s\n",
      "Best mean reward updated -19.895\n",
      "Frame  95554, Episode   87, Mean Reward -19.862, Loss 0.007, Epsilon 23.85%, Elapsed Time 23s\n",
      "Best mean reward updated -19.862\n",
      "Frame  96856, Episode   88, Mean Reward -19.852, Loss 0.007, Epsilon 23.39%, Elapsed Time 22s\n",
      "Best mean reward updated -19.852\n",
      "Frame  98820, Episode   89, Mean Reward -19.809, Loss 0.004, Epsilon 22.71%, Elapsed Time 32s\n",
      "Best mean reward updated -19.809\n",
      "Frame 100430, Episode   90, Mean Reward -19.778, Loss 0.003, Epsilon 22.17%, Elapsed Time 26s\n",
      "Best mean reward updated -19.778\n",
      "Frame 102105, Episode   91, Mean Reward -19.758, Loss 0.003, Epsilon 21.62%, Elapsed Time 27s\n",
      "Best mean reward updated -19.758\n",
      "Frame 103843, Episode   92, Mean Reward -19.728, Loss 0.002, Epsilon 21.06%, Elapsed Time 28s\n",
      "Best mean reward updated -19.728\n",
      "Frame 105438, Episode   93, Mean Reward -19.699, Loss 0.003, Epsilon 20.56%, Elapsed Time 26s\n",
      "Best mean reward updated -19.699\n",
      "Frame 107373, Episode   94, Mean Reward -19.660, Loss 0.005, Epsilon 19.98%, Elapsed Time 30s\n",
      "Best mean reward updated -19.660\n",
      "Frame 108962, Episode   95, Mean Reward -19.653, Loss 0.007, Epsilon 19.51%, Elapsed Time 25s\n",
      "Best mean reward updated -19.653\n",
      "Frame 110728, Episode   96, Mean Reward -19.625, Loss 0.002, Epsilon 19.00%, Elapsed Time 28s\n",
      "Best mean reward updated -19.625\n",
      "Frame 112632, Episode   97, Mean Reward -19.619, Loss 0.003, Epsilon 18.46%, Elapsed Time 30s\n",
      "Best mean reward updated -19.619\n",
      "Frame 114688, Episode   98, Mean Reward -19.582, Loss 0.007, Epsilon 17.90%, Elapsed Time 33s\n",
      "Best mean reward updated -19.582\n",
      "Frame 116669, Episode   99, Mean Reward -19.535, Loss 0.011, Epsilon 17.38%, Elapsed Time 31s\n",
      "Best mean reward updated -19.535\n",
      "Frame 118471, Episode  100, Mean Reward -19.490, Loss 0.001, Epsilon 16.91%, Elapsed Time 28s\n",
      "Best mean reward updated -19.490\n",
      "Frame 120651, Episode  101, Mean Reward -19.460, Loss 0.012, Epsilon 16.37%, Elapsed Time 34s\n",
      "Best mean reward updated -19.460\n",
      "Frame 122597, Episode  102, Mean Reward -19.410, Loss 0.003, Epsilon 15.90%, Elapsed Time 31s\n",
      "Best mean reward updated -19.410\n",
      "Frame 124493, Episode  103, Mean Reward -19.370, Loss 0.004, Epsilon 15.45%, Elapsed Time 30s\n",
      "Best mean reward updated -19.370\n",
      "Frame 126097, Episode  104, Mean Reward -19.360, Loss 0.004, Epsilon 15.09%, Elapsed Time 25s\n",
      "Best mean reward updated -19.360\n",
      "Frame 127971, Episode  105, Mean Reward -19.310, Loss 0.006, Epsilon 14.67%, Elapsed Time 30s\n",
      "Best mean reward updated -19.310\n",
      "Frame 129793, Episode  106, Mean Reward -19.290, Loss 0.005, Epsilon 14.27%, Elapsed Time 29s\n",
      "Best mean reward updated -19.290\n",
      "Frame 132415, Episode  107, Mean Reward -19.230, Loss 0.003, Epsilon 13.72%, Elapsed Time 42s\n",
      "Best mean reward updated -19.230\n",
      "Frame 134749, Episode  108, Mean Reward -19.180, Loss 0.004, Epsilon 13.25%, Elapsed Time 37s\n",
      "Best mean reward updated -19.180\n",
      "Frame 137291, Episode  109, Mean Reward -19.100, Loss 0.004, Epsilon 12.75%, Elapsed Time 42s\n",
      "Best mean reward updated -19.100\n",
      "Frame 139640, Episode  110, Mean Reward -19.060, Loss 0.004, Epsilon 12.31%, Elapsed Time 38s\n",
      "Best mean reward updated -19.060\n",
      "Frame 142263, Episode  111, Mean Reward -18.980, Loss 0.003, Epsilon 11.84%, Elapsed Time 46s\n",
      "Best mean reward updated -18.980\n",
      "Frame 144832, Episode  112, Mean Reward -18.930, Loss 0.003, Epsilon 11.39%, Elapsed Time 44s\n",
      "Best mean reward updated -18.930\n",
      "Frame 147679, Episode  113, Mean Reward -18.860, Loss 0.003, Epsilon 10.91%, Elapsed Time 48s\n",
      "Best mean reward updated -18.860\n",
      "Frame 150132, Episode  114, Mean Reward -18.810, Loss 0.006, Epsilon 10.52%, Elapsed Time 39s\n",
      "Best mean reward updated -18.810\n",
      "Frame 153073, Episode  115, Mean Reward -18.710, Loss 0.003, Epsilon 10.06%, Elapsed Time 50s\n",
      "Best mean reward updated -18.710\n",
      "Frame 155669, Episode  116, Mean Reward -18.640, Loss 0.004, Epsilon 9.68%, Elapsed Time 42s\n",
      "Best mean reward updated -18.640\n",
      "Frame 158021, Episode  117, Mean Reward -18.580, Loss 0.006, Epsilon 9.34%, Elapsed Time 38s\n",
      "Best mean reward updated -18.580\n",
      "Frame 160499, Episode  118, Mean Reward -18.490, Loss 0.007, Epsilon 9.00%, Elapsed Time 41s\n",
      "Best mean reward updated -18.490\n",
      "Frame 162851, Episode  119, Mean Reward -18.450, Loss 0.002, Epsilon 8.69%, Elapsed Time 39s\n",
      "Best mean reward updated -18.450\n",
      "Frame 165823, Episode  120, Mean Reward -18.340, Loss 0.003, Epsilon 8.31%, Elapsed Time 50s\n",
      "Best mean reward updated -18.340\n",
      "Frame 168945, Episode  121, Mean Reward -18.260, Loss 0.002, Epsilon 7.93%, Elapsed Time 50s\n",
      "Best mean reward updated -18.260\n",
      "Frame 171161, Episode  122, Mean Reward -18.220, Loss 0.003, Epsilon 7.67%, Elapsed Time 36s\n",
      "Best mean reward updated -18.220\n",
      "Frame 173762, Episode  123, Mean Reward -18.110, Loss 0.003, Epsilon 7.38%, Elapsed Time 41s\n",
      "Best mean reward updated -18.110\n",
      "Frame 175838, Episode  124, Mean Reward -18.110, Loss 0.004, Epsilon 7.15%, Elapsed Time 32s\n",
      "Frame 178515, Episode  125, Mean Reward -18.020, Loss 0.003, Epsilon 6.87%, Elapsed Time 43s\n",
      "Best mean reward updated -18.020\n",
      "Frame 181266, Episode  126, Mean Reward -17.980, Loss 0.005, Epsilon 6.59%, Elapsed Time 42s\n",
      "Best mean reward updated -17.980\n",
      "Frame 183958, Episode  127, Mean Reward -17.930, Loss 0.003, Epsilon 6.33%, Elapsed Time 42s\n",
      "Best mean reward updated -17.930\n",
      "Frame 186130, Episode  128, Mean Reward -17.910, Loss 0.005, Epsilon 6.13%, Elapsed Time 33s\n",
      "Best mean reward updated -17.910\n",
      "Frame 188716, Episode  129, Mean Reward -17.810, Loss 0.001, Epsilon 5.90%, Elapsed Time 40s\n",
      "Best mean reward updated -17.810\n",
      "Frame 191741, Episode  130, Mean Reward -17.710, Loss 0.005, Epsilon 5.64%, Elapsed Time 46s\n",
      "Best mean reward updated -17.710\n",
      "Frame 194264, Episode  131, Mean Reward -17.600, Loss 0.011, Epsilon 5.43%, Elapsed Time 38s\n",
      "Best mean reward updated -17.600\n",
      "Frame 197078, Episode  132, Mean Reward -17.480, Loss 0.005, Epsilon 5.20%, Elapsed Time 42s\n",
      "Best mean reward updated -17.480\n",
      "Frame 199923, Episode  133, Mean Reward -17.370, Loss 0.002, Epsilon 4.98%, Elapsed Time 43s\n",
      "Best mean reward updated -17.370\n",
      "Frame 202744, Episode  134, Mean Reward -17.240, Loss 0.001, Epsilon 4.78%, Elapsed Time 42s\n",
      "Best mean reward updated -17.240\n",
      "Frame 205783, Episode  135, Mean Reward -17.130, Loss 0.004, Epsilon 4.56%, Elapsed Time 46s\n",
      "Best mean reward updated -17.130\n",
      "Frame 208520, Episode  136, Mean Reward -17.040, Loss 0.004, Epsilon 4.38%, Elapsed Time 41s\n",
      "Best mean reward updated -17.040\n",
      "Frame 210863, Episode  137, Mean Reward -16.980, Loss 0.001, Epsilon 4.23%, Elapsed Time 35s\n",
      "Best mean reward updated -16.980\n",
      "Frame 214079, Episode  138, Mean Reward -16.870, Loss 0.008, Epsilon 4.03%, Elapsed Time 49s\n",
      "Best mean reward updated -16.870\n",
      "Frame 217201, Episode  139, Mean Reward -16.770, Loss 0.006, Epsilon 3.85%, Elapsed Time 47s\n",
      "Best mean reward updated -16.770\n",
      "Frame 220271, Episode  140, Mean Reward -16.650, Loss 0.002, Epsilon 3.67%, Elapsed Time 46s\n",
      "Best mean reward updated -16.650\n",
      "Frame 223229, Episode  141, Mean Reward -16.550, Loss 0.002, Epsilon 3.51%, Elapsed Time 44s\n",
      "Best mean reward updated -16.550\n",
      "Frame 225934, Episode  142, Mean Reward -16.460, Loss 0.001, Epsilon 3.37%, Elapsed Time 40s\n",
      "Best mean reward updated -16.460\n",
      "Frame 229254, Episode  143, Mean Reward -16.340, Loss 0.003, Epsilon 3.21%, Elapsed Time 50s\n",
      "Best mean reward updated -16.340\n",
      "Frame 232844, Episode  144, Mean Reward -16.160, Loss 0.001, Epsilon 3.04%, Elapsed Time 54s\n",
      "Best mean reward updated -16.160\n",
      "Frame 235856, Episode  145, Mean Reward -16.050, Loss 0.004, Epsilon 2.91%, Elapsed Time 45s\n",
      "Best mean reward updated -16.050\n",
      "Frame 238479, Episode  146, Mean Reward -15.990, Loss 0.006, Epsilon 2.80%, Elapsed Time 39s\n",
      "Best mean reward updated -15.990\n",
      "Frame 242303, Episode  147, Mean Reward -15.820, Loss 0.003, Epsilon 2.64%, Elapsed Time 57s\n",
      "Best mean reward updated -15.820\n",
      "Frame 246607, Episode  148, Mean Reward -15.650, Loss 0.001, Epsilon 2.47%, Elapsed Time 05s\n",
      "Best mean reward updated -15.650\n",
      "Frame 249925, Episode  149, Mean Reward -15.530, Loss 0.002, Epsilon 2.35%, Elapsed Time 50s\n",
      "Best mean reward updated -15.530\n",
      "Frame 253547, Episode  150, Mean Reward -15.390, Loss 0.004, Epsilon 2.23%, Elapsed Time 54s\n",
      "Best mean reward updated -15.390\n",
      "Frame 257571, Episode  151, Mean Reward -15.240, Loss 0.002, Epsilon 2.10%, Elapsed Time 00s\n",
      "Best mean reward updated -15.240\n",
      "Frame 261103, Episode  152, Mean Reward -15.080, Loss 0.002, Epsilon 2.00%, Elapsed Time 53s\n",
      "Best mean reward updated -15.080\n",
      "Frame 264816, Episode  153, Mean Reward -14.860, Loss 0.002, Epsilon 2.00%, Elapsed Time 56s\n",
      "Best mean reward updated -14.860\n",
      "Frame 268997, Episode  154, Mean Reward -14.700, Loss 0.003, Epsilon 2.00%, Elapsed Time 03s\n",
      "Best mean reward updated -14.700\n",
      "Frame 272184, Episode  155, Mean Reward -14.540, Loss 0.001, Epsilon 2.00%, Elapsed Time 48s\n",
      "Best mean reward updated -14.540\n",
      "Frame 276012, Episode  156, Mean Reward -14.420, Loss 0.003, Epsilon 2.00%, Elapsed Time 58s\n",
      "Best mean reward updated -14.420\n",
      "Frame 279763, Episode  157, Mean Reward -14.250, Loss 0.002, Epsilon 2.00%, Elapsed Time 56s\n",
      "Best mean reward updated -14.250\n",
      "Frame 283312, Episode  158, Mean Reward -14.110, Loss 0.005, Epsilon 2.00%, Elapsed Time 53s\n",
      "Best mean reward updated -14.110\n",
      "Frame 287210, Episode  159, Mean Reward -13.980, Loss 0.004, Epsilon 2.00%, Elapsed Time 59s\n",
      "Best mean reward updated -13.980\n",
      "Frame 290544, Episode  160, Mean Reward -13.730, Loss 0.002, Epsilon 2.00%, Elapsed Time 50s\n",
      "Best mean reward updated -13.730\n",
      "Frame 293585, Episode  161, Mean Reward -13.480, Loss 0.006, Epsilon 2.00%, Elapsed Time 46s\n",
      "Best mean reward updated -13.480\n",
      "Frame 297566, Episode  162, Mean Reward -13.270, Loss 0.003, Epsilon 2.00%, Elapsed Time 00s\n",
      "Best mean reward updated -13.270\n",
      "Frame 301346, Episode  163, Mean Reward -13.050, Loss 0.001, Epsilon 2.00%, Elapsed Time 57s\n",
      "Best mean reward updated -13.050\n",
      "Frame 305393, Episode  164, Mean Reward -12.820, Loss 0.001, Epsilon 2.00%, Elapsed Time 01s\n",
      "Best mean reward updated -12.820\n",
      "Frame 307879, Episode  165, Mean Reward -12.530, Loss 0.001, Epsilon 2.00%, Elapsed Time 37s\n",
      "Best mean reward updated -12.530\n",
      "Frame 311300, Episode  166, Mean Reward -12.210, Loss 0.002, Epsilon 2.00%, Elapsed Time 51s\n",
      "Best mean reward updated -12.210\n",
      "Frame 313360, Episode  167, Mean Reward -11.840, Loss 0.003, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated -11.840\n",
      "Frame 315658, Episode  168, Mean Reward -11.480, Loss 0.003, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated -11.480\n",
      "Frame 317783, Episode  169, Mean Reward -11.120, Loss 0.002, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -11.120\n",
      "Frame 319669, Episode  170, Mean Reward -10.730, Loss 0.002, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -10.730\n",
      "Frame 321554, Episode  171, Mean Reward -10.380, Loss 0.001, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -10.380\n",
      "Frame 323680, Episode  172, Mean Reward -10.000, Loss 0.009, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -10.000\n",
      "Frame 325767, Episode  173, Mean Reward -9.660, Loss 0.002, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated -9.660\n",
      "Frame 327936, Episode  174, Mean Reward -9.320, Loss 0.002, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -9.320\n",
      "Frame 329956, Episode  175, Mean Reward -8.950, Loss 0.001, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated -8.950\n",
      "Frame 332377, Episode  176, Mean Reward -8.650, Loss 0.001, Epsilon 2.00%, Elapsed Time 36s\n",
      "Best mean reward updated -8.650\n",
      "Frame 334081, Episode  177, Mean Reward -8.260, Loss 0.004, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -8.260\n",
      "Frame 335833, Episode  178, Mean Reward -7.870, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated -7.870\n",
      "Frame 337821, Episode  179, Mean Reward -7.550, Loss 0.001, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated -7.550\n",
      "Frame 339838, Episode  180, Mean Reward -7.180, Loss 0.001, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated -7.180\n",
      "Frame 341548, Episode  181, Mean Reward -6.790, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -6.790\n",
      "Frame 343787, Episode  182, Mean Reward -6.470, Loss 0.001, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated -6.470\n",
      "Frame 346134, Episode  183, Mean Reward -6.170, Loss 0.003, Epsilon 2.00%, Elapsed Time 35s\n",
      "Best mean reward updated -6.170\n",
      "Frame 347837, Episode  184, Mean Reward -5.790, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -5.790\n",
      "Frame 349698, Episode  185, Mean Reward -5.460, Loss 0.000, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -5.460\n",
      "Frame 351547, Episode  186, Mean Reward -5.110, Loss 0.002, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -5.110\n",
      "Frame 353186, Episode  187, Mean Reward -4.730, Loss 0.002, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated -4.730\n",
      "Frame 355079, Episode  188, Mean Reward -4.360, Loss 0.003, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -4.360\n",
      "Frame 357458, Episode  189, Mean Reward -4.090, Loss 0.003, Epsilon 2.00%, Elapsed Time 36s\n",
      "Best mean reward updated -4.090\n",
      "Frame 359231, Episode  190, Mean Reward -3.730, Loss 0.002, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated -3.730\n",
      "Frame 361366, Episode  191, Mean Reward -3.400, Loss 0.001, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -3.400\n",
      "Frame 363193, Episode  192, Mean Reward -3.040, Loss 0.001, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -3.040\n",
      "Frame 364979, Episode  193, Mean Reward -2.680, Loss 0.003, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -2.680\n",
      "Frame 366680, Episode  194, Mean Reward -2.310, Loss 0.000, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -2.310\n",
      "Frame 368805, Episode  195, Mean Reward -1.960, Loss 0.000, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -1.960\n",
      "Frame 371185, Episode  196, Mean Reward -1.670, Loss 0.001, Epsilon 2.00%, Elapsed Time 36s\n",
      "Best mean reward updated -1.670\n",
      "Frame 373085, Episode  197, Mean Reward -1.300, Loss 0.004, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -1.300\n",
      "Frame 374820, Episode  198, Mean Reward -0.930, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated -0.930\n",
      "Frame 376633, Episode  199, Mean Reward -0.600, Loss 0.000, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -0.600\n",
      "Frame 378308, Episode  200, Mean Reward -0.240, Loss 0.002, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -0.240\n",
      "Frame 379948, Episode  201, Mean Reward 0.150, Loss 0.000, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated 0.150\n",
      "Frame 381680, Episode  202, Mean Reward 0.500, Loss 0.000, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 0.500\n",
      "Frame 383442, Episode  203, Mean Reward 0.860, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 0.860\n",
      "Frame 385497, Episode  204, Mean Reward 1.210, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 1.210\n",
      "Frame 387381, Episode  205, Mean Reward 1.560, Loss 0.003, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 1.560\n",
      "Frame 389051, Episode  206, Mean Reward 1.950, Loss 0.002, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 1.950\n",
      "Frame 390996, Episode  207, Mean Reward 2.270, Loss 0.000, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 2.270\n",
      "Frame 392916, Episode  208, Mean Reward 2.600, Loss 0.000, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 2.600\n",
      "Frame 395337, Episode  209, Mean Reward 2.880, Loss 0.002, Epsilon 2.00%, Elapsed Time 36s\n",
      "Best mean reward updated 2.880\n",
      "Frame 397225, Episode  210, Mean Reward 3.250, Loss 0.003, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 3.250\n",
      "Frame 399127, Episode  211, Mean Reward 3.560, Loss 0.001, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 3.560\n",
      "Frame 400890, Episode  212, Mean Reward 3.920, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 3.920\n",
      "Frame 403078, Episode  213, Mean Reward 4.230, Loss 0.002, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated 4.230\n",
      "Frame 404771, Episode  214, Mean Reward 4.580, Loss 0.002, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 4.580\n",
      "Frame 406604, Episode  215, Mean Reward 4.850, Loss 0.001, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 4.850\n",
      "Frame 408244, Episode  216, Mean Reward 5.200, Loss 0.001, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated 5.200\n",
      "Frame 409911, Episode  217, Mean Reward 5.530, Loss 0.000, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 5.530\n",
      "Frame 411612, Episode  218, Mean Reward 5.850, Loss 0.000, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 5.850\n",
      "Frame 413573, Episode  219, Mean Reward 6.200, Loss 0.000, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 6.200\n",
      "Frame 415436, Episode  220, Mean Reward 6.490, Loss 0.001, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 6.490\n",
      "Frame 417273, Episode  221, Mean Reward 6.790, Loss 0.000, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 6.790\n",
      "Frame 419254, Episode  222, Mean Reward 7.120, Loss 0.005, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated 7.120\n",
      "Frame 421604, Episode  223, Mean Reward 7.370, Loss 0.001, Epsilon 2.00%, Elapsed Time 35s\n",
      "Best mean reward updated 7.370\n",
      "Frame 423327, Episode  224, Mean Reward 7.750, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 7.750\n",
      "Frame 425386, Episode  225, Mean Reward 8.000, Loss 0.004, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 8.000\n",
      "Frame 427215, Episode  226, Mean Reward 8.330, Loss 0.009, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 8.330\n",
      "Frame 429302, Episode  227, Mean Reward 8.650, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 8.650\n",
      "Frame 431418, Episode  228, Mean Reward 8.980, Loss 0.001, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated 8.980\n",
      "Frame 433088, Episode  229, Mean Reward 9.280, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 9.280\n",
      "Frame 434727, Episode  230, Mean Reward 9.590, Loss 0.000, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated 9.590\n",
      "Frame 436509, Episode  231, Mean Reward 9.880, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 9.880\n",
      "Frame 438257, Episode  232, Mean Reward 10.170, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 10.170\n",
      "Frame 440265, Episode  233, Mean Reward 10.450, Loss 0.000, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated 10.450\n",
      "Frame 442035, Episode  234, Mean Reward 10.730, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 10.730\n",
      "Frame 443884, Episode  235, Mean Reward 10.990, Loss 0.006, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 10.990\n",
      "Frame 445925, Episode  236, Mean Reward 11.290, Loss 0.000, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 11.290\n",
      "Frame 447583, Episode  237, Mean Reward 11.650, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 11.650\n",
      "Frame 449253, Episode  238, Mean Reward 11.950, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 11.950\n",
      "Frame 451194, Episode  239, Mean Reward 12.250, Loss 0.002, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 12.250\n",
      "Frame 453272, Episode  240, Mean Reward 12.490, Loss 0.000, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 12.490\n",
      "Frame 455066, Episode  241, Mean Reward 12.790, Loss 0.002, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 12.790\n",
      "Frame 457148, Episode  242, Mean Reward 13.060, Loss 0.002, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 13.060\n",
      "Frame 458913, Episode  243, Mean Reward 13.330, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 13.330\n",
      "Frame 460552, Episode  244, Mean Reward 13.570, Loss 0.001, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated 13.570\n",
      "Frame 462312, Episode  245, Mean Reward 13.850, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 13.850\n",
      "Frame 464245, Episode  246, Mean Reward 14.170, Loss 0.001, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 14.170\n",
      "Frame 465902, Episode  247, Mean Reward 14.420, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 14.420\n",
      "Frame 467648, Episode  248, Mean Reward 14.650, Loss 0.000, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 14.650\n",
      "Frame 469367, Episode  249, Mean Reward 14.920, Loss 0.000, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 14.920\n",
      "Frame 471569, Episode  250, Mean Reward 15.150, Loss 0.000, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated 15.150\n",
      "Solved in 471569 frames!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env, device=device)\n",
    "\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "# writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "frame_idx = 0  \n",
    "loss_t = 0\n",
    "\n",
    "best_mean_reward = None\n",
    "start_time = time.time()\n",
    "train_start_time = time.time()\n",
    "\n",
    "print_flag = False\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(epsilon*eps_decay, eps_min)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_time = time.strftime(\"%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "#         print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
    "#             frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "        print(\"Frame {:6},\".format(frame_idx),\n",
    "              \"Episode {:4},\".format(len(total_rewards)),\n",
    "              \"Mean Reward {:.3f},\".format(mean_reward),\n",
    "              \"Loss {:.3f},\".format(loss_t),\n",
    "              \"Epsilon {:.2f}%,\".format(epsilon*100),\n",
    "              \"Elapsed Time {}s\".format(elapsed_time))\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "#         writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "            best_mean_reward = mean_reward\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    # 매 스텝마다 학습\n",
    "    if len(buffer) < replay_start_size:\n",
    "        continue\n",
    "        \n",
    "    batch = buffer.sample(batch_size)\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    actions_v64 = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "#     if print_flag:\n",
    "#         print_flag = False\n",
    "        \n",
    "#         print()\n",
    "#         print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        \n",
    "#         print(\"actions.dtype : {}\".format(actions.dtype))\n",
    "#         print(\"actions.shape : {}\".format(actions.shape))\n",
    "#         print(\"actions_v.dtype : {}\".format(actions_v.dtype))\n",
    "#         print(\"actions_v.shape : {}\".format(actions_v.shape))\n",
    "#         print(\"actions_v64.dtype : {}\".format(actions_v64.dtype))\n",
    "#         print(\"actions_v64.shape : {}\".format(actions_v64.shape))\n",
    "        \n",
    "#         print(\"states_v.dtype : {}\".format(states_v.dtype))\n",
    "#         print(\"states_v.shape : {}\".format(states_v.shape))\n",
    "#         aa = net(states_v)\n",
    "#         print(\"aa.dtype : {}\".format(aa.dtype))\n",
    "#         print(\"aa.shape : {}\".format(aa.shape))\n",
    "#         print(\"actions_v.dtype : {}\".format(actions_v.dtype))\n",
    "#         print(\"actions_v.shape : {}\".format(actions_v.shape))\n",
    "#         bb = actions_v.unsqueeze(-1)\n",
    "#         print(\"bb.dtype : {}\".format(bb.dtype))\n",
    "#         print(\"bb.shape : {}\".format(bb.shape))\n",
    "#         cc = aa.gather(1, bb)\n",
    "#     #     cc = aa.gather(dim=1, index=actions_v)\n",
    "#         print(\"cc.dtype : {}\".format(cc.dtype))\n",
    "#         print(\"cc.shape : {}\".format(cc.shape))\n",
    "#         state_action_values = cc.squeeze(-1)\n",
    "#         print(\"state_action_values.dtype : {}\".format(state_action_values.dtype))\n",
    "#         print(\"state_action_values.shape : {}\".format(state_action_values.shape))\n",
    "        \n",
    "#         print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "#         print()\n",
    "#     else:\n",
    "# #         state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "#         state_action_values = net(states_v).gather(1, actions_v64.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    state_action_values = net(states_v).gather(1, actions_v64.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "\n",
    "    next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "    loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "#     writer.add_scalar(\"loss\", loss_t, frame_idx)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if frame_idx % sync_target_frames == 0:\n",
    "        target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "env.close()\n",
    "\n",
    "training_time = time.time() - train_start_time\n",
    "training_time = time.strftime('%H:%M:%S', time.localtime(training_time))\n",
    "print(\"Training end : {}\".format(training_time))\n",
    "        \n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MZPkszw66cmO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training ends at  2021-08-05 05:57:17.948259\n"
     ]
    }
   ],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WKbcwfK321Hl"
   },
   "outputs": [],
   "source": [
    "# tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [],
   "source": [
    "# # Taken from \n",
    "# # https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "# !apt-get install -y xvfb x11-utils\n",
    "\n",
    "# !pip install pyvirtualdisplay==0.2.* \\\n",
    "#              PyOpenGL==3.1.* \\\n",
    "#              PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "# !pip install gym[box2d]==0.17.*\n",
    "\n",
    "# import pyvirtualdisplay\n",
    "\n",
    "# _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "# _ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BvN4S8R53mJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 6.00\n"
     ]
    }
   ],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"video\"  \n",
    "visualize=True\n",
    "\n",
    "# env = make_env(DEFAULT_ENV_NAME)\n",
    "# env = make_env(\"Pong-v0\")\n",
    "env = make_env(\"PongNoFrameskip-v0\")\n",
    "if record_folder:\n",
    "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pKurZCguxUuQ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = make_env('PongNoFrameskip-v4')\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "while True:\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    state_v = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_v).data.numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
