{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17  \n",
    "- https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af  \n",
    "- https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c\n",
    "- https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "\n",
    "def seed_everything(env, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "#     torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    \n",
    "    \n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        \n",
    "        self.out = 0\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = self.fc(x)\n",
    "        return self.out\n",
    "    \n",
    "    def train(self):\n",
    "        x = torch.rand(4)\n",
    "        x = torch.tensor(x, requires_grad=True)\n",
    "                \n",
    "        loss = self.out\n",
    "        loss /= 10\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_test(env, device=torch.device(\"cpu\")):\n",
    "    print(device)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    model = TestModel()\n",
    "\n",
    "    print(\"random.random() :\", random.random())\n",
    "    print(\"np.random.rand() :\", np.random.rand())\n",
    "    print(\"torch.rand(1) :\", torch.rand(1))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for i in range(100):\n",
    "        x = torch.rand(4)\n",
    "        x = torch.tensor(x, requires_grad=True)\n",
    "\n",
    "        model(x)\n",
    "        loss = model.train()\n",
    "\n",
    "    print(\"model(x) :\", model(x))\n",
    "    print(\"loss :\", loss)\n",
    "\n",
    "\n",
    "    print(np.random.choice(100, 7, replace=False))\n",
    "    \n",
    "    env.reset()\n",
    "    for _ in range(10):\n",
    "        env.reset()\n",
    "        print(env.action_space.sample(), end=\" \")\n",
    "        \n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed_everything(env)\n",
    "\n",
    "device = device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed_test(env, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed_everything(env)\n",
    "\n",
    "device = device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed_test(env, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "## OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "bd0f45bc-3137-4a02-ef13-e71054a16753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QDaXip14JBv",
    "outputId": "cf5337c3-c39d-4774-eb88-f9b094c2e8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uzLQLz04z2i",
    "outputId": "2edf4a8b-3cad-4f76-befa-284baee687c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjUM99rEKFNt",
    "outputId": "d2ddfec6-12b1-45a4-b599-b696672e080c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 05 03:57:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.81       Driver Version: 456.81       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX          WDDM  | 00000000:02:00.0  On |                  N/A |\n",
      "|  0%   45C    P0   149W / 280W |   3142MiB / 24576MiB |     64%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX          WDDM  | 00000000:21:00.0  On |                  N/A |\n",
      "|  0%   41C    P2    66W / 280W |   1868MiB / 24576MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1796    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2668    C+G   ...902.55\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      3048    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      4984    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      5312    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      6720    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7624    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      7732    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8844    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10312    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10968    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10984    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     12784    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     12792    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     13416    C+G   ...xLauncher\\MaxLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     13688    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     13904    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14592    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16296    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     16840    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A     22224    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     22556    C+G   ...nvs\\rlenv_cuda\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     23336    C+G   ...2\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    1   N/A  N/A      1796    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A      3048    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     12784    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     12792    C+G   Insufficient Permissions        N/A      |\n",
      "|    1   N/A  N/A     13416    C+G   ...xLauncher\\MaxLauncher.exe    N/A      |\n",
      "|    1   N/A  N/A     22556      C   ...nvs\\rlenv_cuda\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taYi5LZnIOqz",
    "outputId": "e59ca720-e4ca-4fdc-a5b7-795f4359de43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "# MEAN_REWARD_BOUND = 19.0           \n",
    "MEAN_REWARD_BOUND = 15.0\n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        done_reward = None\n",
    "        \n",
    "#         rr = np.random.random()\n",
    "#         print(\"np.random.random() :\", rr)\n",
    "        if np.random.random() < epsilon:\n",
    "#         if rr < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "#             print(\"state_v.dtype : {}\".format(state_v.dtype))\n",
    "            q_vals_v = net(state_v)\n",
    "#             print(\"q_vals_v.type : {}\".format(q_vals_v.type))\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "#             print(\"act_v.dtype : {}\".format(act_v.dtype))\n",
    "            action = int(act_v.item())\n",
    "#             print(\"type(action) : {}\".format(type(action)))\n",
    "\n",
    "#         print(\"action :\", action)\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "#         print(new_state.shape)\n",
    "#         print(new_state[3, 30:70:5, 8])\n",
    "        \n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipurwYpa6iKn",
    "outputId": "51833adf-bf0a-440b-bb0b-9f12b587f412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2021-08-05 03:57:57.444523\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEoc2PWmM2mu",
    "outputId": "a6e88222-0925-41c5-c20f-2daa316d386e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "random.random() : 0.6394267984578837\n",
      "np.random.rand() : 0.3745401188473625\n",
      "torch.rand(1) : tensor([0.7945])\n",
      "\n",
      "model(x) : tensor([-2.6099, -2.4355, -2.7988], grad_fn=<AddBackward0>)\n",
      "loss : tensor(-0.2533, grad_fn=<MeanBackward0>)\n",
      "[84 55 66 67 45 39 22]\n",
      "3 1 5 5 5 3 2 2 0 0 \n",
      "\n",
      "21-08-05/21:42:18\n",
      "Frame    822, Episode    1, Mean Reward -21.000, Loss 0.000, Epsilon 98.77%, Elapsed Time 00s\n",
      "Best mean reward updated -21.000\n",
      "Frame   1752, Episode    2, Mean Reward -20.500, Loss 0.000, Epsilon 97.41%, Elapsed Time 01s\n",
      "Best mean reward updated -20.500\n",
      "Frame   2574, Episode    3, Mean Reward -20.667, Loss 0.000, Epsilon 96.21%, Elapsed Time 01s\n",
      "Frame   3512, Episode    4, Mean Reward -20.500, Loss 0.000, Epsilon 94.87%, Elapsed Time 01s\n",
      "Frame   4302, Episode    5, Mean Reward -20.600, Loss 0.000, Epsilon 93.75%, Elapsed Time 00s\n",
      "Frame   5064, Episode    6, Mean Reward -20.667, Loss 0.000, Epsilon 92.69%, Elapsed Time 00s\n",
      "Frame   5961, Episode    7, Mean Reward -20.571, Loss 0.000, Epsilon 91.45%, Elapsed Time 01s\n",
      "Frame   6845, Episode    8, Mean Reward -20.625, Loss 0.000, Epsilon 90.24%, Elapsed Time 01s\n",
      "Frame   7727, Episode    9, Mean Reward -20.667, Loss 0.000, Epsilon 89.06%, Elapsed Time 01s\n",
      "Frame   8489, Episode   10, Mean Reward -20.700, Loss 0.000, Epsilon 88.04%, Elapsed Time 01s\n",
      "Frame   9311, Episode   11, Mean Reward -20.727, Loss 0.000, Epsilon 86.96%, Elapsed Time 01s\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "actions.dtype : int32\n",
      "actions.shape : (32,)\n",
      "actions_v.dtype : torch.int64\n",
      "actions_v.shape : torch.Size([32])\n",
      "actions_v64.dtype : torch.int64\n",
      "actions_v64.shape : torch.Size([32])\n",
      "states_v.dtype : torch.float32\n",
      "states_v.shape : torch.Size([32, 4, 84, 84])\n",
      "aa.dtype : torch.float32\n",
      "aa.shape : torch.Size([32, 6])\n",
      "actions_v.dtype : torch.int64\n",
      "actions_v.shape : torch.Size([32])\n",
      "bb.dtype : torch.int64\n",
      "bb.shape : torch.Size([32, 1])\n",
      "cc.dtype : torch.float32\n",
      "cc.shape : torch.Size([32, 1])\n",
      "state_action_values.dtype : torch.float32\n",
      "state_action_values.shape : torch.Size([32])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "Frame  10073, Episode   12, Mean Reward -20.750, Loss 0.031, Epsilon 85.98%, Elapsed Time 01s\n",
      "Frame  10835, Episode   13, Mean Reward -20.769, Loss 0.030, Epsilon 85.00%, Elapsed Time 10s\n",
      "Frame  11755, Episode   14, Mean Reward -20.643, Loss 0.030, Epsilon 83.83%, Elapsed Time 12s\n",
      "Frame  12605, Episode   15, Mean Reward -20.667, Loss 0.059, Epsilon 82.77%, Elapsed Time 11s\n",
      "Frame  13367, Episode   16, Mean Reward -20.688, Loss 0.000, Epsilon 81.83%, Elapsed Time 10s\n",
      "Frame  14129, Episode   17, Mean Reward -20.706, Loss 0.000, Epsilon 80.90%, Elapsed Time 10s\n",
      "Frame  15071, Episode   18, Mean Reward -20.722, Loss 0.026, Epsilon 79.77%, Elapsed Time 12s\n",
      "Frame  16000, Episode   19, Mean Reward -20.737, Loss 0.000, Epsilon 78.66%, Elapsed Time 12s\n",
      "Frame  17204, Episode   20, Mean Reward -20.650, Loss 0.000, Epsilon 77.25%, Elapsed Time 16s\n",
      "Frame  18146, Episode   21, Mean Reward -20.619, Loss 0.001, Epsilon 76.17%, Elapsed Time 12s\n",
      "Frame  18986, Episode   22, Mean Reward -20.591, Loss 0.000, Epsilon 75.22%, Elapsed Time 11s\n",
      "Frame  19973, Episode   23, Mean Reward -20.522, Loss 0.001, Epsilon 74.11%, Elapsed Time 13s\n",
      "Frame  20872, Episode   24, Mean Reward -20.542, Loss 0.001, Epsilon 73.12%, Elapsed Time 12s\n",
      "Frame  21713, Episode   25, Mean Reward -20.560, Loss 0.001, Epsilon 72.20%, Elapsed Time 11s\n",
      "Frame  22632, Episode   26, Mean Reward -20.538, Loss 0.000, Epsilon 71.21%, Elapsed Time 12s\n",
      "Frame  23694, Episode   27, Mean Reward -20.481, Loss 0.004, Epsilon 70.09%, Elapsed Time 14s\n",
      "Best mean reward updated -20.481\n",
      "Frame  24642, Episode   28, Mean Reward -20.500, Loss 0.008, Epsilon 69.10%, Elapsed Time 13s\n",
      "Frame  25555, Episode   29, Mean Reward -20.483, Loss 0.001, Epsilon 68.16%, Elapsed Time 12s\n",
      "Frame  26407, Episode   30, Mean Reward -20.500, Loss 0.004, Epsilon 67.29%, Elapsed Time 11s\n",
      "Frame  27379, Episode   31, Mean Reward -20.516, Loss 0.001, Epsilon 66.32%, Elapsed Time 13s\n",
      "Frame  28263, Episode   32, Mean Reward -20.531, Loss 0.001, Epsilon 65.45%, Elapsed Time 12s\n",
      "Frame  29178, Episode   33, Mean Reward -20.515, Loss 0.001, Epsilon 64.55%, Elapsed Time 13s\n",
      "Frame  30233, Episode   34, Mean Reward -20.471, Loss 0.003, Epsilon 63.54%, Elapsed Time 14s\n",
      "Best mean reward updated -20.471\n",
      "Frame  31199, Episode   35, Mean Reward -20.486, Loss 0.003, Epsilon 62.63%, Elapsed Time 13s\n",
      "Frame  32248, Episode   36, Mean Reward -20.444, Loss 0.001, Epsilon 61.65%, Elapsed Time 14s\n",
      "Best mean reward updated -20.444\n",
      "Frame  33287, Episode   37, Mean Reward -20.432, Loss 0.006, Epsilon 60.70%, Elapsed Time 14s\n",
      "Best mean reward updated -20.432\n",
      "Frame  34459, Episode   38, Mean Reward -20.421, Loss 0.014, Epsilon 59.64%, Elapsed Time 16s\n",
      "Best mean reward updated -20.421\n",
      "Frame  35554, Episode   39, Mean Reward -20.410, Loss 0.003, Epsilon 58.67%, Elapsed Time 15s\n",
      "Best mean reward updated -20.410\n",
      "Frame  36748, Episode   40, Mean Reward -20.400, Loss 0.011, Epsilon 57.62%, Elapsed Time 16s\n",
      "Best mean reward updated -20.400\n",
      "Frame  37877, Episode   41, Mean Reward -20.390, Loss 0.004, Epsilon 56.66%, Elapsed Time 15s\n",
      "Best mean reward updated -20.390\n",
      "Frame  38922, Episode   42, Mean Reward -20.333, Loss 0.002, Epsilon 55.78%, Elapsed Time 14s\n",
      "Best mean reward updated -20.333\n",
      "Frame  40367, Episode   43, Mean Reward -20.256, Loss 0.003, Epsilon 54.58%, Elapsed Time 20s\n",
      "Best mean reward updated -20.256\n",
      "Frame  41701, Episode   44, Mean Reward -20.182, Loss 0.003, Epsilon 53.50%, Elapsed Time 18s\n",
      "Best mean reward updated -20.182\n",
      "Frame  42880, Episode   45, Mean Reward -20.133, Loss 0.005, Epsilon 52.56%, Elapsed Time 16s\n",
      "Best mean reward updated -20.133\n",
      "Frame  44056, Episode   46, Mean Reward -20.130, Loss 0.012, Epsilon 51.64%, Elapsed Time 16s\n",
      "Best mean reward updated -20.130\n",
      "Frame  45364, Episode   47, Mean Reward -20.106, Loss 0.007, Epsilon 50.64%, Elapsed Time 18s\n",
      "Best mean reward updated -20.106\n",
      "Frame  46457, Episode   48, Mean Reward -20.125, Loss 0.005, Epsilon 49.81%, Elapsed Time 15s\n",
      "Frame  47581, Episode   49, Mean Reward -20.122, Loss 0.025, Epsilon 48.98%, Elapsed Time 16s\n",
      "Frame  48615, Episode   50, Mean Reward -20.120, Loss 0.007, Epsilon 48.23%, Elapsed Time 14s\n",
      "Frame  49954, Episode   51, Mean Reward -20.078, Loss 0.003, Epsilon 47.27%, Elapsed Time 19s\n",
      "Best mean reward updated -20.078\n",
      "Frame  51507, Episode   52, Mean Reward -20.019, Loss 0.005, Epsilon 46.18%, Elapsed Time 22s\n",
      "Best mean reward updated -20.019\n",
      "Frame  52870, Episode   53, Mean Reward -19.962, Loss 0.002, Epsilon 45.25%, Elapsed Time 19s\n",
      "Best mean reward updated -19.962\n",
      "Frame  54218, Episode   54, Mean Reward -19.926, Loss 0.018, Epsilon 44.34%, Elapsed Time 19s\n",
      "Best mean reward updated -19.926\n",
      "Frame  55705, Episode   55, Mean Reward -19.927, Loss 0.003, Epsilon 43.36%, Elapsed Time 21s\n",
      "Frame  57035, Episode   56, Mean Reward -19.893, Loss 0.007, Epsilon 42.51%, Elapsed Time 19s\n",
      "Best mean reward updated -19.893\n",
      "Frame  58625, Episode   57, Mean Reward -19.842, Loss 0.003, Epsilon 41.50%, Elapsed Time 23s\n",
      "Best mean reward updated -19.842\n",
      "Frame  60096, Episode   58, Mean Reward -19.810, Loss 0.006, Epsilon 40.60%, Elapsed Time 21s\n",
      "Best mean reward updated -19.810\n",
      "Frame  61576, Episode   59, Mean Reward -19.780, Loss 0.009, Epsilon 39.71%, Elapsed Time 21s\n",
      "Best mean reward updated -19.780\n",
      "Frame  63052, Episode   60, Mean Reward -19.750, Loss 0.007, Epsilon 38.84%, Elapsed Time 21s\n",
      "Best mean reward updated -19.750\n",
      "Frame  64539, Episode   61, Mean Reward -19.721, Loss 0.003, Epsilon 37.98%, Elapsed Time 22s\n",
      "Best mean reward updated -19.721\n",
      "Frame  66189, Episode   62, Mean Reward -19.677, Loss 0.007, Epsilon 37.05%, Elapsed Time 24s\n",
      "Best mean reward updated -19.677\n",
      "Frame  67918, Episode   63, Mean Reward -19.651, Loss 0.004, Epsilon 36.10%, Elapsed Time 25s\n",
      "Best mean reward updated -19.651\n",
      "Frame  69179, Episode   64, Mean Reward -19.625, Loss 0.005, Epsilon 35.43%, Elapsed Time 18s\n",
      "Best mean reward updated -19.625\n",
      "Frame  70464, Episode   65, Mean Reward -19.600, Loss 0.002, Epsilon 34.75%, Elapsed Time 18s\n",
      "Best mean reward updated -19.600\n",
      "Frame  72258, Episode   66, Mean Reward -19.530, Loss 0.004, Epsilon 33.83%, Elapsed Time 26s\n",
      "Best mean reward updated -19.530\n",
      "Frame  74065, Episode   67, Mean Reward -19.493, Loss 0.006, Epsilon 32.92%, Elapsed Time 26s\n",
      "Best mean reward updated -19.493\n",
      "Frame  75626, Episode   68, Mean Reward -19.471, Loss 0.010, Epsilon 32.16%, Elapsed Time 23s\n",
      "Best mean reward updated -19.471\n",
      "Frame  77267, Episode   69, Mean Reward -19.435, Loss 0.004, Epsilon 31.38%, Elapsed Time 23s\n",
      "Best mean reward updated -19.435\n",
      "Frame  78337, Episode   70, Mean Reward -19.457, Loss 0.009, Epsilon 30.88%, Elapsed Time 16s\n",
      "Frame  79784, Episode   71, Mean Reward -19.437, Loss 0.008, Epsilon 30.22%, Elapsed Time 21s\n",
      "Frame  81252, Episode   72, Mean Reward -19.417, Loss 0.008, Epsilon 29.56%, Elapsed Time 21s\n",
      "Best mean reward updated -19.417\n",
      "Frame  82849, Episode   73, Mean Reward -19.425, Loss 0.004, Epsilon 28.86%, Elapsed Time 23s\n",
      "Frame  84624, Episode   74, Mean Reward -19.378, Loss 0.005, Epsilon 28.10%, Elapsed Time 26s\n",
      "Best mean reward updated -19.378\n",
      "Frame  86386, Episode   75, Mean Reward -19.347, Loss 0.017, Epsilon 27.37%, Elapsed Time 26s\n",
      "Best mean reward updated -19.347\n",
      "Frame  88001, Episode   76, Mean Reward -19.303, Loss 0.014, Epsilon 26.71%, Elapsed Time 23s\n",
      "Best mean reward updated -19.303\n",
      "Frame  89581, Episode   77, Mean Reward -19.286, Loss 0.006, Epsilon 26.09%, Elapsed Time 23s\n",
      "Best mean reward updated -19.286\n",
      "Frame  91657, Episode   78, Mean Reward -19.256, Loss 0.003, Epsilon 25.29%, Elapsed Time 30s\n",
      "Best mean reward updated -19.256\n",
      "Frame  93787, Episode   79, Mean Reward -19.190, Loss 0.004, Epsilon 24.49%, Elapsed Time 31s\n",
      "Best mean reward updated -19.190\n",
      "Frame  95056, Episode   80, Mean Reward -19.200, Loss 0.002, Epsilon 24.03%, Elapsed Time 18s\n",
      "Frame  97071, Episode   81, Mean Reward -19.111, Loss 0.008, Epsilon 23.31%, Elapsed Time 29s\n",
      "Best mean reward updated -19.111\n",
      "Frame  99145, Episode   82, Mean Reward -19.049, Loss 0.011, Epsilon 22.60%, Elapsed Time 30s\n",
      "Best mean reward updated -19.049\n",
      "Frame 100623, Episode   83, Mean Reward -19.024, Loss 0.005, Epsilon 22.11%, Elapsed Time 21s\n",
      "Best mean reward updated -19.024\n",
      "Frame 102527, Episode   84, Mean Reward -18.964, Loss 0.004, Epsilon 21.48%, Elapsed Time 28s\n",
      "Best mean reward updated -18.964\n",
      "Frame 104258, Episode   85, Mean Reward -18.906, Loss 0.005, Epsilon 20.93%, Elapsed Time 25s\n",
      "Best mean reward updated -18.906\n",
      "Frame 106161, Episode   86, Mean Reward -18.849, Loss 0.013, Epsilon 20.34%, Elapsed Time 28s\n",
      "Best mean reward updated -18.849\n",
      "Frame 108319, Episode   87, Mean Reward -18.770, Loss 0.006, Epsilon 19.70%, Elapsed Time 31s\n",
      "Best mean reward updated -18.770\n",
      "Frame 110209, Episode   88, Mean Reward -18.716, Loss 0.014, Epsilon 19.14%, Elapsed Time 27s\n",
      "Best mean reward updated -18.716\n",
      "Frame 112137, Episode   89, Mean Reward -18.663, Loss 0.004, Epsilon 18.60%, Elapsed Time 28s\n",
      "Best mean reward updated -18.663\n",
      "Frame 113730, Episode   90, Mean Reward -18.667, Loss 0.011, Epsilon 18.16%, Elapsed Time 23s\n",
      "Frame 115788, Episode   91, Mean Reward -18.604, Loss 0.005, Epsilon 17.61%, Elapsed Time 30s\n",
      "Best mean reward updated -18.604\n",
      "Frame 118244, Episode   92, Mean Reward -18.522, Loss 0.009, Epsilon 16.97%, Elapsed Time 36s\n",
      "Best mean reward updated -18.522\n",
      "Frame 120542, Episode   93, Mean Reward -18.441, Loss 0.002, Epsilon 16.40%, Elapsed Time 34s\n",
      "Best mean reward updated -18.441\n",
      "Frame 123094, Episode   94, Mean Reward -18.351, Loss 0.006, Epsilon 15.78%, Elapsed Time 37s\n",
      "Best mean reward updated -18.351\n",
      "Frame 125119, Episode   95, Mean Reward -18.337, Loss 0.008, Epsilon 15.31%, Elapsed Time 30s\n",
      "Best mean reward updated -18.337\n",
      "Frame 126518, Episode   96, Mean Reward -18.333, Loss 0.006, Epsilon 14.99%, Elapsed Time 21s\n",
      "Best mean reward updated -18.333\n",
      "Frame 129118, Episode   97, Mean Reward -18.227, Loss 0.004, Epsilon 14.42%, Elapsed Time 39s\n",
      "Best mean reward updated -18.227\n",
      "Frame 131189, Episode   98, Mean Reward -18.173, Loss 0.002, Epsilon 13.98%, Elapsed Time 31s\n",
      "Best mean reward updated -18.173\n",
      "Frame 133834, Episode   99, Mean Reward -18.081, Loss 0.005, Epsilon 13.43%, Elapsed Time 40s\n",
      "Best mean reward updated -18.081\n",
      "Frame 136930, Episode  100, Mean Reward -17.960, Loss 0.003, Epsilon 12.82%, Elapsed Time 46s\n",
      "Best mean reward updated -17.960\n",
      "Frame 139507, Episode  101, Mean Reward -17.870, Loss 0.011, Epsilon 12.34%, Elapsed Time 38s\n",
      "Best mean reward updated -17.870\n",
      "Frame 141886, Episode  102, Mean Reward -17.800, Loss 0.004, Epsilon 11.90%, Elapsed Time 36s\n",
      "Best mean reward updated -17.800\n",
      "Frame 144515, Episode  103, Mean Reward -17.700, Loss 0.003, Epsilon 11.44%, Elapsed Time 39s\n",
      "Best mean reward updated -17.700\n",
      "Frame 146470, Episode  104, Mean Reward -17.640, Loss 0.002, Epsilon 11.11%, Elapsed Time 29s\n",
      "Best mean reward updated -17.640\n",
      "Frame 148909, Episode  105, Mean Reward -17.540, Loss 0.004, Epsilon 10.71%, Elapsed Time 36s\n",
      "Best mean reward updated -17.540\n",
      "Frame 151190, Episode  106, Mean Reward -17.470, Loss 0.009, Epsilon 10.35%, Elapsed Time 35s\n",
      "Best mean reward updated -17.470\n",
      "Frame 153862, Episode  107, Mean Reward -17.350, Loss 0.005, Epsilon 9.95%, Elapsed Time 40s\n",
      "Best mean reward updated -17.350\n",
      "Frame 156825, Episode  108, Mean Reward -17.170, Loss 0.010, Epsilon 9.51%, Elapsed Time 44s\n",
      "Best mean reward updated -17.170\n",
      "Frame 159427, Episode  109, Mean Reward -17.060, Loss 0.004, Epsilon 9.15%, Elapsed Time 39s\n",
      "Best mean reward updated -17.060\n",
      "Frame 162231, Episode  110, Mean Reward -16.960, Loss 0.003, Epsilon 8.77%, Elapsed Time 42s\n",
      "Best mean reward updated -16.960\n",
      "Frame 164196, Episode  111, Mean Reward -16.920, Loss 0.004, Epsilon 8.52%, Elapsed Time 30s\n",
      "Best mean reward updated -16.920\n",
      "Frame 166961, Episode  112, Mean Reward -16.810, Loss 0.002, Epsilon 8.17%, Elapsed Time 42s\n",
      "Best mean reward updated -16.810\n",
      "Frame 169293, Episode  113, Mean Reward -16.750, Loss 0.004, Epsilon 7.89%, Elapsed Time 35s\n",
      "Best mean reward updated -16.750\n",
      "Frame 172361, Episode  114, Mean Reward -16.640, Loss 0.005, Epsilon 7.54%, Elapsed Time 46s\n",
      "Best mean reward updated -16.640\n",
      "Frame 174944, Episode  115, Mean Reward -16.550, Loss 0.004, Epsilon 7.25%, Elapsed Time 39s\n",
      "Best mean reward updated -16.550\n",
      "Frame 177093, Episode  116, Mean Reward -16.480, Loss 0.002, Epsilon 7.02%, Elapsed Time 32s\n",
      "Best mean reward updated -16.480\n",
      "Frame 179973, Episode  117, Mean Reward -16.330, Loss 0.002, Epsilon 6.72%, Elapsed Time 43s\n",
      "Best mean reward updated -16.330\n",
      "Frame 181951, Episode  118, Mean Reward -16.260, Loss 0.002, Epsilon 6.53%, Elapsed Time 29s\n",
      "Best mean reward updated -16.260\n",
      "Frame 184798, Episode  119, Mean Reward -16.150, Loss 0.001, Epsilon 6.25%, Elapsed Time 43s\n",
      "Best mean reward updated -16.150\n",
      "Frame 187733, Episode  120, Mean Reward -16.020, Loss 0.005, Epsilon 5.98%, Elapsed Time 44s\n",
      "Best mean reward updated -16.020\n",
      "Frame 191319, Episode  121, Mean Reward -15.850, Loss 0.002, Epsilon 5.67%, Elapsed Time 53s\n",
      "Best mean reward updated -15.850\n",
      "Frame 194933, Episode  122, Mean Reward -15.680, Loss 0.004, Epsilon 5.37%, Elapsed Time 54s\n",
      "Best mean reward updated -15.680\n",
      "Frame 197288, Episode  123, Mean Reward -15.610, Loss 0.004, Epsilon 5.19%, Elapsed Time 35s\n",
      "Best mean reward updated -15.610\n",
      "Frame 199956, Episode  124, Mean Reward -15.480, Loss 0.003, Epsilon 4.98%, Elapsed Time 40s\n",
      "Best mean reward updated -15.480\n",
      "Frame 203705, Episode  125, Mean Reward -15.250, Loss 0.003, Epsilon 4.71%, Elapsed Time 56s\n",
      "Best mean reward updated -15.250\n",
      "Frame 206944, Episode  126, Mean Reward -15.080, Loss 0.003, Epsilon 4.49%, Elapsed Time 48s\n",
      "Best mean reward updated -15.080\n",
      "Frame 209524, Episode  127, Mean Reward -15.010, Loss 0.005, Epsilon 4.32%, Elapsed Time 38s\n",
      "Best mean reward updated -15.010\n",
      "Frame 212890, Episode  128, Mean Reward -14.860, Loss 0.001, Epsilon 4.10%, Elapsed Time 50s\n",
      "Best mean reward updated -14.860\n",
      "Frame 215675, Episode  129, Mean Reward -14.620, Loss 0.003, Epsilon 3.94%, Elapsed Time 42s\n",
      "Best mean reward updated -14.620\n",
      "Frame 218788, Episode  130, Mean Reward -14.400, Loss 0.006, Epsilon 3.76%, Elapsed Time 47s\n",
      "Best mean reward updated -14.400\n",
      "Frame 221829, Episode  131, Mean Reward -14.210, Loss 0.002, Epsilon 3.59%, Elapsed Time 46s\n",
      "Best mean reward updated -14.210\n",
      "Frame 225799, Episode  132, Mean Reward -13.990, Loss 0.003, Epsilon 3.38%, Elapsed Time 59s\n",
      "Best mean reward updated -13.990\n",
      "Frame 228620, Episode  133, Mean Reward -13.730, Loss 0.003, Epsilon 3.24%, Elapsed Time 42s\n",
      "Best mean reward updated -13.730\n",
      "Frame 231202, Episode  134, Mean Reward -13.450, Loss 0.004, Epsilon 3.12%, Elapsed Time 39s\n",
      "Best mean reward updated -13.450\n",
      "Frame 234015, Episode  135, Mean Reward -13.290, Loss 0.009, Epsilon 2.99%, Elapsed Time 42s\n",
      "Best mean reward updated -13.290\n",
      "Frame 236157, Episode  136, Mean Reward -12.940, Loss 0.003, Epsilon 2.89%, Elapsed Time 32s\n",
      "Best mean reward updated -12.940\n",
      "Frame 239334, Episode  137, Mean Reward -12.700, Loss 0.005, Epsilon 2.76%, Elapsed Time 48s\n",
      "Best mean reward updated -12.700\n",
      "Frame 241759, Episode  138, Mean Reward -12.380, Loss 0.005, Epsilon 2.66%, Elapsed Time 36s\n",
      "Best mean reward updated -12.380\n",
      "Frame 244151, Episode  139, Mean Reward -12.080, Loss 0.005, Epsilon 2.57%, Elapsed Time 36s\n",
      "Best mean reward updated -12.080\n",
      "Frame 246655, Episode  140, Mean Reward -11.760, Loss 0.001, Epsilon 2.47%, Elapsed Time 37s\n",
      "Best mean reward updated -11.760\n",
      "Frame 249330, Episode  141, Mean Reward -11.440, Loss 0.001, Epsilon 2.38%, Elapsed Time 40s\n",
      "Best mean reward updated -11.440\n",
      "Frame 251910, Episode  142, Mean Reward -11.160, Loss 0.002, Epsilon 2.29%, Elapsed Time 39s\n",
      "Best mean reward updated -11.160\n",
      "Frame 254032, Episode  143, Mean Reward -10.830, Loss 0.007, Epsilon 2.21%, Elapsed Time 32s\n",
      "Best mean reward updated -10.830\n",
      "Frame 256065, Episode  144, Mean Reward -10.500, Loss 0.003, Epsilon 2.15%, Elapsed Time 31s\n",
      "Best mean reward updated -10.500\n",
      "Frame 257887, Episode  145, Mean Reward -10.130, Loss 0.001, Epsilon 2.09%, Elapsed Time 27s\n",
      "Best mean reward updated -10.130\n",
      "Frame 259877, Episode  146, Mean Reward -9.760, Loss 0.003, Epsilon 2.03%, Elapsed Time 30s\n",
      "Best mean reward updated -9.760\n",
      "Frame 262042, Episode  147, Mean Reward -9.400, Loss 0.006, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -9.400\n",
      "Frame 264140, Episode  148, Mean Reward -9.040, Loss 0.004, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated -9.040\n",
      "Frame 266484, Episode  149, Mean Reward -8.710, Loss 0.001, Epsilon 2.00%, Elapsed Time 35s\n",
      "Best mean reward updated -8.710\n",
      "Frame 268647, Episode  150, Mean Reward -8.360, Loss 0.002, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -8.360\n",
      "Frame 270604, Episode  151, Mean Reward -8.000, Loss 0.002, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated -8.000\n",
      "Frame 272372, Episode  152, Mean Reward -7.640, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated -7.640\n",
      "Frame 274384, Episode  153, Mean Reward -7.310, Loss 0.004, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated -7.310\n",
      "Frame 276674, Episode  154, Mean Reward -6.990, Loss 0.001, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated -6.990\n",
      "Frame 278785, Episode  155, Mean Reward -6.610, Loss 0.004, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -6.610\n",
      "Frame 280755, Episode  156, Mean Reward -6.260, Loss 0.007, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated -6.260\n",
      "Frame 282908, Episode  157, Mean Reward -5.920, Loss 0.002, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -5.920\n",
      "Frame 285075, Episode  158, Mean Reward -5.610, Loss 0.003, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated -5.610\n",
      "Frame 287876, Episode  159, Mean Reward -5.320, Loss 0.002, Epsilon 2.00%, Elapsed Time 42s\n",
      "Best mean reward updated -5.320\n",
      "Frame 289934, Episode  160, Mean Reward -4.970, Loss 0.002, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated -4.970\n",
      "Frame 292514, Episode  161, Mean Reward -4.680, Loss 0.001, Epsilon 2.00%, Elapsed Time 38s\n",
      "Best mean reward updated -4.680\n",
      "Frame 294777, Episode  162, Mean Reward -4.410, Loss 0.001, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated -4.410\n",
      "Frame 296606, Episode  163, Mean Reward -4.040, Loss 0.002, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -4.040\n",
      "Frame 298257, Episode  164, Mean Reward -3.650, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -3.650\n",
      "Frame 300205, Episode  165, Mean Reward -3.310, Loss 0.001, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated -3.310\n",
      "Frame 302188, Episode  166, Mean Reward -3.000, Loss 0.002, Epsilon 2.00%, Elapsed Time 30s\n",
      "Best mean reward updated -3.000\n",
      "Frame 304043, Episode  167, Mean Reward -2.640, Loss 0.003, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated -2.640\n",
      "Frame 306147, Episode  168, Mean Reward -2.310, Loss 0.002, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated -2.310\n",
      "Frame 307970, Episode  169, Mean Reward -1.960, Loss 0.001, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -1.960\n",
      "Frame 309921, Episode  170, Mean Reward -1.590, Loss 0.001, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated -1.590\n",
      "Frame 311589, Episode  171, Mean Reward -1.210, Loss 0.000, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated -1.210\n",
      "Frame 313566, Episode  172, Mean Reward -0.860, Loss 0.002, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated -0.860\n",
      "Frame 315202, Episode  173, Mean Reward -0.450, Loss 0.003, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated -0.450\n",
      "Frame 316991, Episode  174, Mean Reward -0.100, Loss 0.000, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated -0.100\n",
      "Frame 319225, Episode  175, Mean Reward 0.210, Loss 0.001, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated 0.210\n",
      "Frame 320995, Episode  176, Mean Reward 0.570, Loss 0.001, Epsilon 2.00%, Elapsed Time 26s\n",
      "Best mean reward updated 0.570\n",
      "Frame 323291, Episode  177, Mean Reward 0.900, Loss 0.001, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated 0.900\n",
      "Frame 325362, Episode  178, Mean Reward 1.220, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 1.220\n",
      "Frame 327030, Episode  179, Mean Reward 1.570, Loss 0.003, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 1.570\n",
      "Frame 328941, Episode  180, Mean Reward 1.950, Loss 0.001, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 1.950\n",
      "Frame 331019, Episode  181, Mean Reward 2.210, Loss 0.012, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 2.210\n",
      "Frame 332806, Episode  182, Mean Reward 2.540, Loss 0.001, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 2.540\n",
      "Frame 335011, Episode  183, Mean Reward 2.840, Loss 0.003, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated 2.840\n",
      "Frame 336980, Episode  184, Mean Reward 3.130, Loss 0.002, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 3.130\n",
      "Frame 338645, Episode  185, Mean Reward 3.480, Loss 0.001, Epsilon 2.00%, Elapsed Time 25s\n",
      "Best mean reward updated 3.480\n",
      "Frame 340595, Episode  186, Mean Reward 3.780, Loss 0.001, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 3.780\n",
      "Frame 342503, Episode  187, Mean Reward 4.080, Loss 0.001, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 4.080\n",
      "Frame 344143, Episode  188, Mean Reward 4.430, Loss 0.001, Epsilon 2.00%, Elapsed Time 24s\n",
      "Best mean reward updated 4.430\n",
      "Frame 346695, Episode  189, Mean Reward 4.670, Loss 0.002, Epsilon 2.00%, Elapsed Time 38s\n",
      "Best mean reward updated 4.670\n",
      "Frame 348499, Episode  190, Mean Reward 5.050, Loss 0.001, Epsilon 2.00%, Elapsed Time 27s\n",
      "Best mean reward updated 5.050\n",
      "Frame 350326, Episode  191, Mean Reward 5.360, Loss 0.001, Epsilon 2.00%, Elapsed Time 28s\n",
      "Best mean reward updated 5.360\n",
      "Frame 352169, Episode  192, Mean Reward 5.650, Loss 0.001, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated 5.650\n",
      "Frame 354067, Episode  193, Mean Reward 5.950, Loss 0.004, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated 5.950\n",
      "Frame 355798, Episode  194, Mean Reward 6.250, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 6.250\n",
      "Frame 357647, Episode  195, Mean Reward 6.600, Loss 0.001, Epsilon 2.00%, Elapsed Time 33s\n",
      "Best mean reward updated 6.600\n",
      "Frame 360066, Episode  196, Mean Reward 6.900, Loss 0.003, Epsilon 2.00%, Elapsed Time 44s\n",
      "Best mean reward updated 6.900\n",
      "Frame 362116, Episode  197, Mean Reward 7.150, Loss 0.001, Epsilon 2.00%, Elapsed Time 36s\n",
      "Best mean reward updated 7.150\n",
      "Frame 364049, Episode  198, Mean Reward 7.470, Loss 0.057, Epsilon 2.00%, Elapsed Time 34s\n",
      "Best mean reward updated 7.470\n",
      "Frame 365700, Episode  199, Mean Reward 7.770, Loss 0.006, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 7.770\n",
      "Frame 367439, Episode  200, Mean Reward 8.030, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 8.030\n",
      "Frame 369176, Episode  201, Mean Reward 8.350, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 8.350\n",
      "Frame 370953, Episode  202, Mean Reward 8.680, Loss 0.001, Epsilon 2.00%, Elapsed Time 31s\n",
      "Best mean reward updated 8.680\n",
      "Frame 372759, Episode  203, Mean Reward 8.980, Loss 0.000, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated 8.980\n",
      "Frame 374543, Episode  204, Mean Reward 9.300, Loss 0.001, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated 9.300\n",
      "Frame 376362, Episode  205, Mean Reward 9.610, Loss 0.001, Epsilon 2.00%, Elapsed Time 32s\n",
      "Best mean reward updated 9.610\n",
      "Frame 378313, Episode  206, Mean Reward 9.940, Loss 0.002, Epsilon 2.00%, Elapsed Time 35s\n",
      "Best mean reward updated 9.940\n",
      "Frame 379966, Episode  207, Mean Reward 10.230, Loss 0.000, Epsilon 2.00%, Elapsed Time 29s\n",
      "Best mean reward updated 10.230\n",
      "Frame 382159, Episode  208, Mean Reward 10.420, Loss 0.001, Epsilon 2.00%, Elapsed Time 39s\n",
      "Best mean reward updated 10.420\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "seed_everything(env)\n",
    "\n",
    "seed_test(env, device=device)\n",
    "\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "# writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "frame_idx = 0  \n",
    "loss_t = 0\n",
    "\n",
    "best_mean_reward = None\n",
    "start_time = time.time()\n",
    "train_start_time = time.time()\n",
    "\n",
    "print_flag = True\n",
    "\n",
    "current_time = time.strftime('%y-%m-%d/%X', time.localtime(time.time()))\n",
    "print(current_time)\n",
    "    \n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(epsilon*eps_decay, eps_min)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_time = time.strftime(\"%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "#         print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
    "#             frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "        print(\"Frame {:6},\".format(frame_idx),\n",
    "              \"Episode {:4},\".format(len(total_rewards)),\n",
    "              \"Mean Reward {:.3f},\".format(mean_reward),\n",
    "              \"Loss {:.3f},\".format(loss_t),\n",
    "              \"Epsilon {:.2f}%,\".format(epsilon*100),\n",
    "              \"Elapsed Time {}s\".format(elapsed_time))\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "#         writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "#         writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "#         writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "            best_mean_reward = mean_reward\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    # 매 스텝마다 학습\n",
    "    if len(buffer) < replay_start_size:\n",
    "        continue\n",
    "        \n",
    "    batch = buffer.sample(batch_size)\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "#     states_v = torch.tensor(states, device=device)\n",
    "#     next_states_v = torch.tensor(next_states, device=device)\n",
    "#     actions_v = torch.tensor(actions, device=device)\n",
    "#     actions_v64 = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "#     rewards_v = torch.tensor(rewards, device=device)\n",
    "#     done_mask = torch.cuda.ByteTensor(dones)\n",
    "\n",
    "    states_v = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    next_states_v = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "    actions_v = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "    actions_v64 = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "    rewards_v = torch.tensor(rewards, device=device)\n",
    "    done_mask = torch.cuda.ByteTensor(dones)\n",
    "\n",
    "    if print_flag:\n",
    "        print_flag = False\n",
    "        \n",
    "        print()\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        \n",
    "        print(\"actions.dtype : {}\".format(actions.dtype))\n",
    "        print(\"actions.shape : {}\".format(actions.shape))\n",
    "        print(\"actions_v.dtype : {}\".format(actions_v.dtype))\n",
    "        print(\"actions_v.shape : {}\".format(actions_v.shape))\n",
    "        print(\"actions_v64.dtype : {}\".format(actions_v64.dtype))\n",
    "        print(\"actions_v64.shape : {}\".format(actions_v64.shape))\n",
    "        \n",
    "        print(\"states_v.dtype : {}\".format(states_v.dtype))\n",
    "        print(\"states_v.shape : {}\".format(states_v.shape))\n",
    "        aa = net(states_v)\n",
    "        print(\"aa.dtype : {}\".format(aa.dtype))\n",
    "        print(\"aa.shape : {}\".format(aa.shape))\n",
    "        print(\"actions_v.dtype : {}\".format(actions_v.dtype))\n",
    "        print(\"actions_v.shape : {}\".format(actions_v.shape))\n",
    "        bb = actions_v.unsqueeze(-1)\n",
    "        print(\"bb.dtype : {}\".format(bb.dtype))\n",
    "        print(\"bb.shape : {}\".format(bb.shape))\n",
    "        cc = aa.gather(1, bb)\n",
    "    #     cc = aa.gather(dim=1, index=actions_v)\n",
    "        print(\"cc.dtype : {}\".format(cc.dtype))\n",
    "        print(\"cc.shape : {}\".format(cc.shape))\n",
    "        state_action_values = cc.squeeze(-1)\n",
    "        print(\"state_action_values.dtype : {}\".format(state_action_values.dtype))\n",
    "        print(\"state_action_values.shape : {}\".format(state_action_values.shape))\n",
    "        \n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print()\n",
    "    else:\n",
    "#         state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        state_action_values = net(states_v).gather(1, actions_v64.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "#     state_action_values = net(states_v).gather(1, actions_v64.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "\n",
    "    next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "    loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "#     writer.add_scalar(\"loss\", loss_t, frame_idx)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if frame_idx % sync_target_frames == 0:\n",
    "        target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "env.close()\n",
    "\n",
    "training_time = time.time() - train_start_time\n",
    "training_time = time.strftime('%H:%M:%S', time.gmtime(training_time))\n",
    "print(\"Training end : {}\".format(training_time))\n",
    "        \n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MZPkszw66cmO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training ends at  2021-08-05 05:57:17.948259\n"
     ]
    }
   ],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WKbcwfK321Hl"
   },
   "outputs": [],
   "source": [
    "# tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [],
   "source": [
    "# # Taken from \n",
    "# # https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "# !apt-get install -y xvfb x11-utils\n",
    "\n",
    "# !pip install pyvirtualdisplay==0.2.* \\\n",
    "#              PyOpenGL==3.1.* \\\n",
    "#              PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "# !pip install gym[box2d]==0.17.*\n",
    "\n",
    "# import pyvirtualdisplay\n",
    "\n",
    "# _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "# _ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BvN4S8R53mJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 6.00\n"
     ]
    }
   ],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"video\"  \n",
    "visualize=True\n",
    "\n",
    "# env = make_env(DEFAULT_ENV_NAME)\n",
    "# env = make_env(\"Pong-v0\")\n",
    "env = make_env(\"PongNoFrameskip-v0\")\n",
    "if record_folder:\n",
    "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pKurZCguxUuQ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = make_env('PongNoFrameskip-v4')\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "while True:\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    state_v = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_v).data.numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
